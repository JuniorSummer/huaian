1.在Hadoop上，安装并配置Hive数据仓库软件。完成后，启动Hive服务，创建“web”数据库，执行指令“show databases;”并输出相应结果。

hive

create database if not exists web;
show databases;

2.创建库表并查看输出相应字段信息

use web;
DROP TABLE IF EXISTS ip; 
CREATE TABLE ip (
    ip_start STRING COMMENT 'Start IP',
    ip_end STRING COMMENT 'End IP',
    location STRING COMMENT 'Location',
    isp STRING COMMENT 'ISP information'
)
COMMENT 'IP address information table'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t';
DESCRIBE ip;
-- 会有更详细信息
DESCRIBE formatted ip;


-- 解释
在Hive中，LOAD DATA INPATH 是一个用来将HDFS上的数据文件加载到Hive表中的命令。
LOAD DATA INPATH: 这是HiveQL（Hive查询语言）的一个语句，用于将指定的HDFS路径中的数据加载到Hive表中。
'/data/ip_processed.txt': 这是HDFS上的文件路径，指向你想要加载到Hive表中的数据文件。这个文件应该包含与Hive表中列相对应的数据。
INTO TABLE ip: 这指定了要将数据加载到的Hive表的名称，在这个例子中，表名为 ip。
执行这个命令后，Hive会将 /data/ip_processed.txt 文件中的数据移动到Hive表的指定位置，并且数据会按照Hive表的schema进行解析。
注意以下几点：
使用 LOAD DATA INPATH 时，数据文件 /data/ip_processed.txt 必须位于HDFS上，而不是本地文件系统。
文件的内容必须与Hive表的结构相匹配，否则加载可能会失败或者数据可能不会正确解析。
执行这个命令时，源文件 /data/ip_processed.txt 会被移动到Hive仓库内部，并且原始文件会从HDFS上的原始位置被删除。
如果Hive表已经包含了数据，那么加载的数据将会被追加到表中。
如果你想要保留原始文件，可以使用 LOAD DATA LOCAL INPATH 命令，它会复制数据而不是移动数据。

DROP TABLE IF EXISTS log; 
CREATE TABLE log (
    id BIGINT COMMENT 'Log ID',
    ip STRING COMMENT 'User IP address',
    access_time STRING COMMENT 'Access time',
    access_url STRING COMMENT 'Access URL',
    status INT COMMENT 'Status code',
    traffic BIGINT COMMENT 'Traffic generated by the access',
    source_url STRING COMMENT 'Referrer URL'
)
COMMENT 'Table to store web access logs'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t';
DESCRIBE log;



3.导入数据到Hive表中
LOAD DATA INPATH '/data/ip_processed.txt' INTO TABLE ip;
LOAD DATA INPATH '/data/log_processed.txt' INTO TABLE log;

'''做这个题有时候会把之前的/root/service/yunan/result文件夹清空，如果报错说hdfs上没有这个文件，就把ip_processed.txt和log_processed.txt重新上传一遍
cd /root/service/yunan/result
hdfs dfs -put ip_processed.txt /data
hdfs dfs -put log_processed.txt /data
# 结果检测
SELECT COUNT(*) FROM ip;
SELECT COUNT(*) FROM log;
# ip输出结果应为 446721，log输出结果应为 61279
'''


# 启动spark会需要很久，而且不能多开spark，但第一次启动后，后面的运算就很快了；如果有报错可以把对应的comment都加上
DROP TABLE IF EXISTS ip_log_num;
CREATE TABLE ip_log_num (
type STRING,
num INT
);
INSERT OVERWRITE TABLE ip_log_num
SELECT 'log', count(*) AS num FROM web.log WHERE log.status='301'
UNION ALL
SELECT 'ip', count(*) FROM web.ip WHERE ip.location='IANA';

# 如果上面的验证没法通过，就换成下面的（验证规则还能变啊）
DROP TABLE IF EXISTS ip_log_num;
CREATE TABLE ip_log_num (
type STRING,
num INT
);
INSERT INTO TABLE ip_log_num
SELECT 'ip' AS type, count(*) AS num FROM web.log WHERE status='301';



4.统计“该网站用户访问次数最多的10个页面”
DROP TABLE IF EXISTS asses_url_top;
CREATE TABLE asses_url_top (
    rank INT,
    access_url STRING,
    times INT
);
INSERT OVERWRITE TABLE asses_url_top
SELECT
    -- 使用窗口函数生成按访问次数降序排列的次序列，并命名为rank
    ROW_NUMBER() OVER (ORDER BY COUNT(*) DESC) AS rank, 
    -- 选择访问链接url
    access_url, 
    -- 选择访问次数，并将其命名为times
    COUNT(*) AS times 
-- 从log表中选择数据
FROM log 
-- 按url分组，统计每个url的访问次数
GROUP BY access_url 
-- 按访问次数降序排列
ORDER BY times DESC 
-- 选择访问次数最多的前10条记录
LIMIT 10;

'''结果
USE web;
select * from web.asses_url_top;

asses_url_top.rank      asses_url_top.access_url        asses_url_top.times
1       /shop/updateDurationVideo       3212
2       /       2931
3       /i18nBrowse?userLocale=zh_CN&r=1276     2279
4       /portal/user/hasmessage 1674
5       /portal/queryUserNoPayRecord    1666
6       /shop/pre/next  955
7       /shop/show/desc 612
8       /item/detail    610
9       /home/index     530
10      /js/umeditor/editor.min.js?1276 366
Time taken: 0.115 seconds, Fetched: 10 row(s)
'''

5.网站跳转来源类型统计（从这题开始老师都不讲了）

DROP TABLE IF EXISTS source_url_info; 
CREATE TABLE source_url_info (
    source VARCHAR(50),
    count INT,
    ratio DECIMAL(10, 2)
);
INSERT OVERWRITE TABLE source_url_info
SELECT
    source, -- 选择来源类型
    COUNT(*) AS count, -- 统计每种来源类型的数量
    COUNT(*) * 100.0 / total.total_count AS ratio -- 计算每种来源类型占总数的比例
FROM (
    -- 子查询：对每条日志的来源链接进行分类
    SELECT
        CASE
            -- 当来源链接包含 'www.pdd.com' 时，分类为 'Access via PDD'
            WHEN INSTR(source_url, 'www.pdd.com') > 0 THEN 'Access via PDD'
            -- 当来源链接包含 'www.google.com' 时，分类为 'Access via Google'
            WHEN INSTR(source_url, 'www.google.com') > 0 THEN 'Access via Google'
            -- 当来源链接包含 'www.baidu.com' 时，分类为 'Access via Baidu'
            WHEN INSTR(source_url, 'www.baidu.com') > 0 THEN 'Access via Baidu'
            -- 当来源链接长度小于 5 时，分类为 'Access directly via URL'
            WHEN LENGTH(source_url) < 5 THEN 'Access directly via URL'
            -- 其他情况，分类为 'Access via other means'
            ELSE 'Access via other means'
        END AS source
    FROM log
) AS categorized_log
CROSS JOIN (
    -- 子查询：统计日志表 log 中的总记录数
    SELECT COUNT(*) AS total_count FROM log
) AS total
-- 按来源类型和总记录数进行分组
GROUP BY source, total.total_count;


'''结果
select * from web.source_url_info;

source_url_info.source  source_url_info.count   source_url_info.ratio
Access via PDD  53390   87.13
Access via Baidu        87      0.14
Access via Google       10      0.02
Access directly via URL 5153    8.41
Access via other means  2639    4.31
Time taken: 0.094 seconds, Fetched: 5 row(s)
'''


6.统计网站PV与UV
DROP TABLE IF EXISTS pv_uv; 
CREATE TABLE pv_uv (
    hour STRING, -- 小时
    pv BIGINT, -- 浏览量
    uv BIGINT -- 访客数
);
INSERT OVERWRITE TABLE pv_uv
SELECT
    SUBSTR(access_time, 13, 2) AS hour, -- 提取小时信息
    COUNT(*) AS pv, -- 计算浏览量
    COUNT(DISTINCT ip) AS uv -- 计算访客数
FROM log
WHERE access_time LIKE '22/Aug/2017%' -- 过滤指定日期
GROUP BY SUBSTR(access_time, 13, 2) -- 按小时分组
ORDER BY hour -- 按小时排序;

'''
select * from web.pv_uv;

pv_uv.hour	pv_uv.pv	pv_uv.uv
00	1123	77
01	644	50
02	470	52
03	351	47
04	324	56
05	537	45
06	1433	55
07	586	47
08	1065	46
09	4110	113
10	4179	99
11	3124	95
12	1348	67
13	1812	71
14	4582	94
15	6659	152
16	6831	128
17	7041	104
18	4790	87
19	1729	64
20	1611	74
21	2767	86
22	2343	80
23	1813	58
'''

7.统计来自“北京市东城区”的浏览记录数量
比赛可以先试试直接上传ip_num.txt文件，在mapreduce的yunan中

-- 创建log_tmp 表，存储log 表ip地址对应的十进制数
DROP TABLE IF EXISTS log_tmp; 
CREATE TABLE log_tmp (
    ip BIGINT
);
INSERT OVERWRITE TABLE log_tmp
SELECT
    SPLIT(ip, '\\.')[0] * 256 * 256 * 256 +
    SPLIT(ip, '\\.')[1] * 256 * 256 +
    SPLIT(ip, '\\.')[2] * 256 +
    SPLIT(ip, '\\.')[3] AS ip
FROM log;
-- 创建ip_tmp 表，存储“北京市东城区”对应的所有ip地址的十进制数
DROP TABLE IF EXISTS ip_tmp; 
CREATE TABLE ip_tmp (
    ip_start BIGINT,
    ip_end BIGINT,
    location STRING
);
INSERT OVERWRITE TABLE ip_tmp
SELECT
    SPLIT(ip_start, '\\.')[0] * 256 * 256 * 256 +
    SPLIT(ip_start, '\\.')[1] * 256 * 256 +
    SPLIT(ip_start, '\\.')[2] * 256 +
    SPLIT(ip_start, '\\.')[3] AS ip_start,
    SPLIT(ip_end, '\\.')[0] * 256 * 256 * 256 +
    SPLIT(ip_end, '\\.')[1] * 256 * 256 +
    SPLIT(ip_end, '\\.')[2] * 256 +
    SPLIT(ip_end, '\\.')[3] AS ip_end,
    location
FROM ip
WHERE
    location LIKE '%北京市东城区%';
-- 查询log_tmp 表中属于“北京市东城区”的浏览记录数量
DROP TABLE IF EXISTS ip_num; 
CREATE TABLE ip_num (
    ip_num BIGINT
);
INSERT OVERWRITE TABLE ip_num
SELECT
    COUNT(l.ip) as ip_num
FROM
    log_tmp l
JOIN
    ip_tmp i
ON
    l.ip >= i.ip_start AND l.ip <= i.ip_end;
-- 保存数据至本地
INSERT OVERWRITE LOCAL DIRECTORY '/root/service/yunan/result/'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
SELECT ip_num FROM web.ip_num;

'''
select * from web.ip_num;
54
'''

mv  /root/service/yunan/result/*  /root/service/yunan/result/ip_num.txt
cat  /root/service/yunan/result/ip_num.txt