**************************************************************************************************************************
###系统服务开启
#注意一步步验证，因为涉及进程的重启

1.修改云主机host，修改hadoop映射
vim /etc/hosts
#复制内网地址进去

#免密操作
# ssh localhost
ssh hadoop000

'''可选：改名
hostnamectl set-hostname hadoop000 
bash
ssh-keygen
ssh copyid做免密
'''

2&3.格式化HDFS文件系统，启动hadoop集群（会有两个yes验证）
hdfs namenode -format
start-dfs.sh && start-yarn.sh
jps

'''
# 或一次性都启动
start-all.sh 
'''

4.模拟故障

# 删除对应文件；也是必须要删的
rm -rf /root/hadoopData/*

'''
先停掉这个进程（虽然可能验证可以通过，但还是要模拟一下，不然下一步无法通过，有点坑；如果已经没有namenode就不用kill了；
jps
kill -9 对应PID

# 不行就先去xml里看看路径对不对（cd $HADOOP_HOME，cd etc/hadoop/）
# 找到hdfs-namenode存储地址
cd /root/software/hadoop-2.7.7/etc/hadoop
cat core-site.xml
'''


5&6.恢复namenode，比赛里一般直接格式化就行
stop-all.sh
hdfs namenode -format
start-dfs.sh && start-yarn.sh

'''
# 不行的话就说明里面本来有数据，要从secondaryNameNode下备份
cd /root/hadoopData/temp/dfs/namesecondary/
# 拷贝内容到
cp /root/hadoopData/temp/dfs/namesecondary/in_use.lock hadoopData/name/
'''

7.离开安全模式(进入就是把最后换成enter；要解决完问题才可以leave，不然系统会自动进入安全模式)
hdfs dfsadmin -safemode leave

'''
## 可选：通过指令查看状态
hdfs dfsadmin -safemode get
## 可选：忘记了可以查看hdfs下命令
hdfs dfsadmin
# hadoop2.7.7可以进入 互联网地址:5070端口查看集群信息
#hadoop3就是9870端口
看safemode状态
'''


**************************************************************************************************************************
###mapreduce分析与优化

1.上传文件到hdfs
cd /root/data/
hadoop fs -put mobile.txt /
hadoop fs -ls /

2.
# 通过xftp上传文件part-r-00000到/root/data/目录下或   vim part-r-00000
# 复制结果并上传到hdfs
hadoop fs -mkdir /mobile
hadoop fs -put ./mobile/part-r-00000 /mobile

'''
cd /root/data
python 商城手机数据分析.py
'''

3.将HDFS上结果文件保存至本地/root/data目录下（其实已完成了）
cp part-r-00000 /root/data/

'''
如果不行就：hdfs dfs -get /mobile /root/data
## hdfs上删除文件 hadoop fs -rm /mobile/part-r-00000

# 追求速度可以用python中的pandas来解决
# 注意要把输出改名为part-r-00000（mapreduce默认输出名
## 可选；使用java或者scala+spark来解决
'''


**************************************************************************************************************************
###hive分析与优化：后面几个hive执行很慢，可以多开几个hive同步执行语句
hive中函数查询：describe function substr;


1.开启mysql服务
systemctl start mysqld
systemctl status mysqld

'''
## 查看配置参数:cd $HIVE_HOME + cd conf/
cd /root/software/apache-hive-2.3.4-bin/conf
vim hive-site.xml
'''

2.初始化数据库，出现“schemaTool completed”表示创建成功;# 后台启动hive客户端
schematool -dbType mysql -initSchema
nohup hive --service metastore > /dev/null 2>&1 &
hive

create database if not exists shop;
show databases;

'''
## 可选：查看当前状态
jps
jobs
netstat -lanp | grep 9083
创建shop数据库；也可以直接输命令hive -e "create database if not exists shop;show databases"
'''

**********************************************************************************
取巧方法：
3-7.
默认创建的hive表在hdfs:/user/hive/warehouse下
把本地shop.db文件夹上传到 /root/data 下
hdfs dfs -ls /user/hive/warehouse
hdfs dfs -put shop.db/* /user/hive/warehouse/shop.db/
hdfs dfs -ls /user/hive/warehouse/shop.db

6和7需要创建对应的表才能验证通过
use shop;
drop table if exists clicklog;
create external table if not exists clicklog (
user_id string,
user_ip string,
product_id string,
click_time string,
action_type string,
area_id string
) row format delimited fields terminated by ',';
use shop;
create external table if not exists area_hot_product (
area_id string,
area_name string,
product_id string,
product_name string,
pv bigint
) row format delimited fields terminated by ',';

8.
把本地area_hot_product文件夹上传到/root/data/shop/下即可

'''
使用这个方法拷贝之后，hive里是读不到对应的table的，所以不是那么完美的方法。比赛真遇到了可以试试，还是能节约很多时间
'''
*******************************************************************************
正经路子
3.创建表并加载数据（注意文件路径变化）

use shop;
drop table if exists product;
create external table if not exists product (
product_id string,
product_name string,
marque string,
barcode string,
price double,
brand_id string,
market_price double,
stock int,
status int
) row format delimited fields terminated by ',';
load data local inpath '/root/data/shop/product.txt' into table product;

'''
## 可选：查询、退出、删除
select * from product limit 10;
exit;
drop table shop;
show databases;
show tables;
'''


4.创建area表格并加载数据

use shop;
create external table if not exists area (
area_id string,
area_name string
) row format delimited fields terminated by ',';
load data local inpath '/root/data/shop/area.txt' into table area;


5.创建user_click表格并加载数据

use shop;
create external table if not exists user_click (
user_id string,
user_ip string,
url string,
click_time string,
action_type string,
area_id string
) row format delimited fields terminated by ',';
load data local inpath '/root/data/shop/user_click.txt' into table user_click;


6.创建clicklog表格，解析user_click用户点击信息表中的product_id

use shop;
drop table if exists clicklog;
create external table if not exists clicklog (
user_id string,
user_ip string,
product_id string,
click_time string,
action_type string,
area_id string
) row format delimited fields terminated by ',';
insert overwrite table clicklog
select user_id, user_ip, substring(url, instr(url, '=')+1) as
product_id, click_time, action_type, area_id from user_click;

7.创建area_hot_product表;统计各地区热门商品访问量
use shop;
create external table if not exists area_hot_product (
area_id string,
area_name string,
product_id string,
product_name string,
pv bigint
) row format delimited fields terminated by ',';
insert overwrite table area_hot_product
select t1.area_id, area_name, t1.product_id, product_name, count(t1.product_id) as pv
from clicklog as t1 join area as t2
on t1.area_id = t2.area_id join product t3
on t1.product_id=t3.product_id
group by t1.area_id, area_name, t1.product_id, product_name;

8.查询表area_hot_product，写入到本地目录
# 可以直接xftp把 area_hot_product 下的结果进行上传

insert overwrite local directory '/root/data/shop/area_hot_product'
row format delimited fields terminated by ','
select * from area_hot_product;